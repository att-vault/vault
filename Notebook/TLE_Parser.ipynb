{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLE Parser\n",
    "\n",
    "### Domain Specific Packaged Used\n",
    "\n",
    "* https://pypi.org/project/ephem/ \n",
    "* https://federicostra.github.io/tletools/ \n",
    "\n",
    "### Use\n",
    "This notebook includes various functions to validate and or correct the Air Force TLE data and produce gridded data capable of ingesting into the compute engine\n",
    "\n",
    "This notebook assumes that data is stored at the following locations relative to this notebook\n",
    "- TLE data to be validated is stored at `../data/TLE/source`\n",
    "- TLEs that cant be validated are witten directly to a file at `../data/TLE/errors`\n",
    "- Validated TLE data is written at `../data/TLE/processed`\n",
    "\n",
    "\n",
    "\n",
    "Valid data is written in CSV format using the following schema \n",
    "\n",
    "| Norad ID | Epoch Year | Epoch JD | TLE |   |\n",
    "|----------|------------|----------|-----|---|\n",
    "| INT      | INT        | FLOAT    | STR |   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ephem\n",
    "import pathlib\n",
    "from tletools.tle import TLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WorkingFolder = pathlib.Path.cwd().joinpath('../data/TLE')\n",
    "source_files = list(WorkingFolder.joinpath('source').glob('*.txt'))\n",
    "error_filess = list(WorkingFolder.joinpath('errors').glob('*.e'))\n",
    "processed_files = list(WorkingFolder.joinpath('processed').glob('*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_validate_tle_files(files):\n",
    "    \"\"\"\n",
    "        This function takes a list of TLE files and validates each TLE in them using the Pyephem and TLE-tools library, \n",
    "        TLEs that can't be validated are written back to an error file directory as `<inputfilename>.e` for further processing\n",
    "    \"\"\"\n",
    "    \n",
    "    for file in files:\n",
    "        print(f'processing {file.name}')\n",
    "        with open(file) as tle_file:\n",
    "            with open(WorkingFolder.joinpath(f'errors/{file.name[:-4]}.e'), 'w+') as error_file:\n",
    "                i = 0\n",
    "                tracks = []\n",
    "\n",
    "                while True:\n",
    "                    # try to create a pyephem and TLE-tools objects, if fail write TLE string to error file and proceed to next TLE\n",
    "                    try:\n",
    "                        # print to see progress\n",
    "                        i += 1\n",
    "                        if i % 500000 == 0:\n",
    "                            print(i)\n",
    "\n",
    "                        # compose TLE string to process in TLE-Tool object    \n",
    "                        name = 'None\\n'\n",
    "                        firstline = tle_file.readline().replace('\\\\', '') # extra slash at the end of line one can cause a parsing error\n",
    "                        secondline = tle_file.readline()\n",
    "\n",
    "                        # detect if EOF\n",
    "                        if firstline == '':\n",
    "                            break\n",
    "\n",
    "                        # run through pyephem for checksum detection \n",
    "                        ephem.readtle(name, firstline, secondline)\n",
    "\n",
    "                        #run through tle-tools for easy accessing norad_id, epoch year, epoch jd\n",
    "                        track = TLE.from_lines(name, firstline.strip(), secondline.strip()).asdict()\n",
    "                        trackdict = {\n",
    "                            'norad_id': track['norad'],\n",
    "                            'epoch_year': track['epoch_year'],\n",
    "                            'epoch_day': track['epoch_day'],\n",
    "                            'tle': f\"\"\"{name}{firstline}{secondline}\"\"\"\n",
    "                        }\n",
    "                        tracks.append(trackdict)\n",
    "\n",
    "                    except:\n",
    "                        # write error\n",
    "                        error_file.write(firstline)\n",
    "                        error_file.write(secondline)\n",
    "\n",
    "            print(f'completed processing {file.name}')    \n",
    "\n",
    "            # create DataFrame and output to CSV\n",
    "            df = pd.DataFrame(tracks)\n",
    "            df.to_csv(f'../data/TLE/processed/{file.name[:-4]}.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inital Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_validate_tle_files(source_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}